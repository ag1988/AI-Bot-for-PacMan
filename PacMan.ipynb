{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural net for playing PacMan using Q-Learning\n",
    "## Author : Ankit Gupta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before proceeding please install 'OpenAI Gym' \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anime\n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "action_space : Discrete(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAGfCAYAAADbBoDgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJkBAaAIEhCZA4H+hIkTGWcltwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9bc970550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# starting a new game and plotting the frame\n",
    "\n",
    "obs = env.reset()\n",
    "print(obs.shape) #[ht, wd, channels]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(obs)\n",
    "plt.axis('off')\n",
    "\n",
    "# 9 possible actions in Pacman : 0,...,8\n",
    "# 0: no change, 1: up, 2: right, 3: left, 4: down, ...\n",
    "print('action_space :', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original obs shape : (210, 160, 3)\n",
      "new obs shape : (84, 80, 3)\n",
      "0 228 -128 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 79.5, 83.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAAD8CAYAAABAfImTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABrFJREFUeJzt3T2S3EQYBuBdiktQhhtQxEQUByAj\nIeIMm8ERIJszEJGQcQCKiJhyQoxdHGMIXAZ7rPH8SN399avnCXdnR2pp3v1U3dJ8j8fj8QHI8MHo\nHQC2I9AQRKAhiEBDEIGGIAINQQQaggg0BBFoCPJhz429fHpyWxrc6dnh8HjpNSo0BBFoCNL1knvJ\nx798NHoX7vbi63/e+dnM45lZ2rlYGs81VGgIItAQRKAhiEBDkOGTYkvOTQgsTXJcOxlyy6TJ1hMs\nM4+nxXbWvO59279G2nhOqdAQ5LHnd4ot3SmWtrQw83hmlnYulsbjTjHYGYGGIAINQQQaggg0BBFo\nCCLQECTyTrFrVVynXDOemaWdi1HjUaEhiEBDEIGGIAINQUpOio028vFJ/jf68ckZqdAQRKAhiEvu\nBVtfgiVd0vXU4rilnwsVGoIINAQpecmdflkErajQEESgIYhAQxCBhiAlJ8VuMXICzTppHc7FKyo0\nBBFoCCLQEESgIUjJSbFZ2q+2aCOqnez7X9fiPSuO514qNAQRaAiiPzQUpD80INCQRKAhiEBDEIGG\nIAINQYbfKXZLy85qHRPWtn5t0Y2jmpFjbHF+q58zFRqCCDQEEWgIItAQZPik2Fq9Jrt63XOe3u70\n4SHv+FY6Zyo0BBFoCDL9JXfa17emXV4vSTu+lc6ZCg1BBBqCCDQEEWgIItAQRKAhiEBDkK7r0JXW\n67aQsJ76x6e/Lf788+dfbvL+a/i8vO14uPwaFRqCTH+nGPd5XZnPVeJLv6cmFRqCCDQEKXnJ3aKN\n6Nrtp7VFffHX4ks3387Mx+1a2skCTQg0BOnaTvbxkx/7bexEpa+JqeDc+vOpPc9yV/vMHP/+TjtZ\n2BOBhiACDUEEGoKUXIemvT1PdiVToSFI1wo9uj3oqdEtWau3Jt3CHsZ4Le1kgZsINAQRaAgi0BBE\noCHI9OvQ1W6gX2vNeEaP+9pZXOesHRUagkxfoWf+z74kbTxL0sZYaTwqNAQRaAgi0BBEoCGIQEMQ\ngYYgAg1BSraTbfHcaKW1Qq6zl3N27Ti1k4WdEWgIItAQRKAhyPQPZ/Rqv1qtLerMZmknO7JF7b1U\naAgi0BCkZDvZHp3uZ7fVZd6v3/68+POvfvpm2D6lWvu51k4Wdmb6STHu87oyn6vEl35PTSo0BBFo\nCCLQEESgIYhAQ5Cp+kPPuu1zKqzRnluHrmAv52zLcarQEESgIYhAQxCBhiDT3/qZ9kBAr/GMvKXT\nOWtHhYYg01fomf+zL1kznorLPEucs3ZUaAgi0BBEoCGIQEMQgYYgAg1BBBqClPwa3xYq3c1TyZ8/\n/P7e33/2/Red9qSeap8ZX+MLOyPQEGT6Wz/ZzpuX15cuxalJhYYgJSv06ParM7ST3aot6qVJr7Q2\nvNU+L1ts/00qNAQRaAhiHZr/LE2EWYd+m3VooBuBhiAlZ7np59x6854vtWemQkMQFXrnVOIsKjQE\nEWgIMlU72WrrxiNvI1z7nmvMMsYZPi9bU6EhiEBDEIGGIAINQaZfh55hgqXFc7AVJ4H2etwqnQsV\nGoIINASZ/pK7xaXN1u85wz5uYa/HrdK5UKEhiEBDEIGGIAINQQQaggg0BBm+bNVryr/S0kJvex77\nGiOX4e591FKFhiACDUEEGoIINAQZPim2ZHQb0bTHANeMx3G7/FrtZIEmBBqCdG0n+/Lp6Z2NzbxG\nmv7tF604bpctjefZ4aCdLOyJQEMQgYYgAg1BBBqCCDQEEWgIUvLWz1usadFZcZ2yxT5Vaydb8bgv\nmfGzpUJDEIGGIAINQQQagpScFOvx3Oit2+/1XG+L57t7meH54ZHHzfPQwE0EGoKUvOQefek48vuY\nr33t6GO0ZOt9T2v92mPbKjQEEWgIItAQRKAhiEBDEIGGICWXrUbfKbbG1o8qPjzMMe5btDhGvKJC\nQxCBhiACDUEEGoIINAQpOcs9WsVnjU9V3MeK+3Rqhn1cQ4WGICr0ghn+Y1fcx4r7dGqGfVxDhYYg\nAg1BSl5yp18WQSsqNAQRaAgi0BBEoCFIyUmxW6R/LesWqu1ntf05Z5b9fJMKDUEEGoIINAQRaAhS\nclJsdPvVkW1RR45n7d/u9biNbI97SoWGIAINQR6Px2O3jb18enpnYzOu9UFrS5fnzw6Hx0t/p0JD\nEIGGIAINQQQagnSdFAPaUqEhiEBDEIGGIAINQQQaggg0BBFoCCLQEESgIYhAQxCBhiACDUEEGoII\nNAQRaAgi0BBEoCGIQEMQgYYgAg1BBBqCCDQEEWgI8i8hdBiVnzEYCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb996529160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reducing the image size, crop for faster training\n",
    "\n",
    "def preprocess(obs):\n",
    "    img = obs[1:168:2, : : 2, :] # crop and downsize\n",
    "    img = img - 128 # normalize between -128 and 127\n",
    "    img = img.astype('int8') # saves memory\n",
    "    return img\n",
    "\n",
    "print('original obs shape :', obs.shape)\n",
    "print('new obs shape :', preprocess(obs).shape)\n",
    "print(obs.min(), obs.max(), preprocess(obs).min(), preprocess(obs).max())\n",
    "\n",
    "plt.imshow((preprocess(obs) + 128).astype('uint8')) # plt doesn't plot 'int8'\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constructing the online and target Q-nets\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "input_height = 84\n",
    "input_width = 80\n",
    "n_channels = 3 \n",
    "conv_n_maps = [32, 64, 64]\n",
    "conv_kernel_sizes = [(8, 8), (4, 4), (3, 3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_pads = ['SAME']*3\n",
    "conv_activation = [tf.nn.relu]*3\n",
    "n_hidden_in = conv_n_maps[2] * 11 * 10 # conv3 has 64 maps of 11x10 each\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "\n",
    "# we only allow 4 actions : 1: up, 2: right, 3: left, 4: down\n",
    "n_outputs = 4  # (in output vector, map is changed to 0: up, 1: right, 2: left, 3: down) \n",
    "\n",
    "# He-initializer\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "outputs_initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as we have to build 2 identitical Q-nets, lets define a func for it \n",
    "\n",
    "def q_net(X_inp, name):\n",
    "    X_inp_normalized = tf.cast(X_inp, dtype='float32') / 128.0\n",
    "    \n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # all vars names will be prefixed with 'name'\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(X_inp_normalized, filters=conv_n_maps[0], kernel_size=conv_kernel_sizes[0], \n",
    "                                 strides=conv_strides[0], padding=conv_pads[0], \n",
    "                                 activation=conv_activation[0], kernel_initializer=he_init)\n",
    "        conv2 = tf.layers.conv2d(conv1, filters=conv_n_maps[1], kernel_size=conv_kernel_sizes[1], \n",
    "                                 strides=conv_strides[1], padding=conv_pads[1], \n",
    "                                 activation=conv_activation[1], kernel_initializer=he_init)\n",
    "        conv3 = tf.layers.conv2d(conv2, filters=conv_n_maps[2], kernel_size=conv_kernel_sizes[2], \n",
    "                                 strides=conv_strides[2], padding=conv_pads[2], \n",
    "                                 activation=conv_activation[2], kernel_initializer=he_init)\n",
    "        hidden = tf.layers.dense(tf.reshape(conv3, shape=(-1, n_hidden_in)), \n",
    "                                 n_hidden, activation=hidden_activation, kernel_initializer=he_init)\n",
    "        outputs = tf.layers.dense(hidden, n_outputs, kernel_initializer=outputs_initializer)\n",
    "        \n",
    "        trainbale_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        \n",
    "    return outputs, trainbale_vars\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_state = tf.placeholder(dtype='int8', shape=(None, input_height, input_width, n_channels), name='X')\n",
    "\n",
    "q_vals_online, vars_online = q_net(X_state, 'online')\n",
    "q_vals_target, vars_target = q_net(X_state, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function for copying online vars to target vars\n",
    "\n",
    "assign_ops = []\n",
    "\n",
    "for i, var in enumerate(vars_target):\n",
    "    assign_ops.append(tf.assign(var, vars_online[i]))\n",
    "    \n",
    "copy_online_to_target = tf.group(*assign_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining loss func and training ops\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X_action = tf.placeholder('uint8', shape=(None)) # actions taken by online q-net\n",
    "y = tf.placeholder('float32', shape=(None, 1))  # q-val estimates using target q-net\n",
    "\n",
    "q_val_action = tf.reduce_sum(q_vals_online * tf.one_hot(X_action, n_outputs),\n",
    "                            axis=1, keep_dims=True) #[batch_size, 1]\n",
    "\n",
    "# using L1 loss as in L2 loss the gradients are too unstable\n",
    "loss = tf.reduce_mean(tf.abs(q_val_action - y)) # L1 loss\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# checking for exploding grads\n",
    "bottom_grads = optimizer.compute_gradients(loss, var_list=[vars_online[0]])\n",
    "bottom_grads_norm = tf.norm(bottom_grads, ord=1) # reflects changes in smaller gradients\n",
    "\n",
    "#saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#init\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining helper functions and game memory\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "max_mem_size = 2000\n",
    "game_memory = deque([], max_mem_size) # a queue with max len\n",
    "\n",
    "epsilon0 = 1.0\n",
    "epsilon_min = 0.9\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(step, q_vals_eval):\n",
    "    # this is more relevant if you have the patience for training the net for a LONG time\n",
    "    #unfortunately we will not be doing so: hence epsilon_min is kept very high\n",
    "    epsilon = max(epsilon_min, epsilon0 - (epsilon0 - epsilon_min)*(step / eps_decay_steps))\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(n_outputs)\n",
    "    else:\n",
    "        return np.argmax(q_vals_eval)\n",
    "    \n",
    "def get_next_batch(batch_size, n_steps_train):\n",
    "    indices = np.random.permutation(-np.arange(1, n_steps_train))[:batch_size]\n",
    "    (batch_state, batch_action, batch_reward, batch_next_state, batch_continue) = ([], [], [], [], [])\n",
    "    for idx in indices:\n",
    "        state, action, reward, next_state, cont = game_memory[idx]     \n",
    "        batch_state.append(state)\n",
    "        batch_action.append(action)\n",
    "        batch_reward.append(reward)\n",
    "        batch_next_state.append(next_state)\n",
    "        batch_continue.append(cont)\n",
    "    batch_state = np.array(batch_state)\n",
    "    batch_next_state = np.array(batch_next_state)\n",
    "    batch_action = np.array(batch_action).reshape(batch_size, 1)\n",
    "    batch_reward = np.array(batch_reward).reshape(batch_size, 1)\n",
    "    batch_continue = np.array(batch_continue).reshape(batch_size, 1)\n",
    "    return batch_state, batch_action, batch_reward, batch_next_state, batch_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting a training session \n",
    "\n",
    "n_steps = 5000000     # num steps to train for\n",
    "n_steps_train = 15     # train online q-net after these many steps\n",
    "n_steps_copy = 3000   # copy online net to target after these many steps\n",
    "discount_rate = 0.999999 \n",
    "batch_size = 5 \n",
    "skip_start = 85       # skip the start of every game (it's just waiting time in PacMan)\n",
    "training_start = 60   # start training after these many steps \n",
    "timer = 8             # update action after these life steps, this was determined using experiments\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "\n",
    "step = 0\n",
    "done = True # has a game ended?\n",
    "game_memory = deque([], max_mem_size)\n",
    "\n",
    "# vars for keeping track of training progress\n",
    "opt_actions= deque([], n_steps_train)\n",
    "game_reward = 0  # total reward scored in a game\n",
    "n_games = 0\n",
    "last_10_losses = deque([], 10)  # a queue for storing last 10 training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 : 1.0 {2: 15} 700.769  600 : 1.3 {1: 15} 739.582  900 : 0.9 {1: 15} 704.016  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7f0c027ed009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         loss_eval, _, b_grads_norm = sess.run([loss, training_op, bottom_grads_norm], \n\u001b[1;32m     55\u001b[0m                                              feed_dict={X_state: batch_state, \n\u001b[0;32m---> 56\u001b[0;31m                                                         X_action: batch_action.reshape(-1), y: q_est}\n\u001b[0m\u001b[1;32m     57\u001b[0m                                             )\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# queue the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "while step < n_steps:\n",
    "    step += 1\n",
    "    \n",
    "    if done:\n",
    "        # start a  new game\n",
    "        obs = env.reset()\n",
    "        # skip the start of each game as in pacman its just waiting\n",
    "        for skip in range(skip_start): \n",
    "                obs, reward, done, info = env.step(0)\n",
    "        state = preprocess(obs)\n",
    "        lives_left = 3\n",
    "        game_reward = 0\n",
    "        life_step = 0\n",
    "        n_games += 1\n",
    "        \n",
    "    \n",
    "    # determine the action using e-greedy policy on online q_net\n",
    "    q_vals_eval = q_vals_online.eval(feed_dict={X_state: state.reshape(1, input_height, input_width, n_channels)})[0]\n",
    "    opt_actions.append(1 + np.argmax(q_vals_eval))\n",
    "    \n",
    "    # update the action every 8 life steps\n",
    "    if life_step % timer == 0:\n",
    "        action = epsilon_greedy(step, q_vals_eval)\n",
    "            \n",
    "    #execute the action\n",
    "    (obs, reward, done, info) = env.step(action + 1)\n",
    "    next_state = preprocess(obs)\n",
    "    life_step += 1\n",
    "    game_reward += int(reward)\n",
    "    \n",
    "    # record this step in game memory\n",
    "    game_memory.append((state, action, reward, next_state, 1 - done))\n",
    "    \n",
    "    # skip the start of each new life as in pacman its just waiting\n",
    "    if lives_left != info['ale.lives']:\n",
    "        lives_left = info['ale.lives']\n",
    "        life_step = 0\n",
    "        for skip in range(35): \n",
    "            obs, reward, done, info = env.step(0)\n",
    "    \n",
    "    if step < training_start:\n",
    "        # there are not enough game memories to train usefully\n",
    "        continue\n",
    "        \n",
    "    if step % n_steps_train == 0:\n",
    "        # train the online q_net \n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_continue = get_next_batch(batch_size, n_steps_train)\n",
    "        # estimate the q_online vals using reward and q_target val\n",
    "        q_target_eval = q_vals_target.eval(feed_dict={X_state: batch_next_state}) # [batch_size, n_outputs]\n",
    "        q_target_eval_max = np.max(q_target_eval, axis=1).reshape(-1, 1) # [batch_size, 1]\n",
    "        q_est = batch_reward + discount_rate * q_target_eval_max * batch_continue # [batch_size, 1]\n",
    "        loss_eval, _, b_grads_norm = sess.run([loss, training_op, bottom_grads_norm], \n",
    "                                             feed_dict={X_state: batch_state, \n",
    "                                                        X_action: batch_action.reshape(-1), y: q_est}\n",
    "                                            )\n",
    "        # queue the loss\n",
    "        last_10_losses.append(loss_eval)\n",
    "        \n",
    "        if step % 300 == 0:\n",
    "            # print the mean of last 10 train losses, bottom grads norm , etc\n",
    "            print(step, ':', '%.1f' % np.mean(last_10_losses), dict(Counter(opt_actions)), b_grads_norm, end='  ')\n",
    "            opt_actions= deque([], n_steps_train)\n",
    "            \n",
    "        # exploding bottom layer grads\n",
    "        if loss_eval > 2e14 or b_grads_norm > 2e14:\n",
    "            print('-------------DIVERGED----------------')\n",
    "            break\n",
    "        \n",
    "    if step % n_steps_copy == 0:\n",
    "        # copy online q_net vars to target q_net\n",
    "        sess.run(copy_online_to_target)\n",
    "        print('|copy|', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store the model\n",
    "\n",
    "saver.save(sess, './datasets/Pacman/pacman.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 2 4 4 3 1 4 1 1 1 2 4 2 4 3 1 4 4 4 4 1 4 4 4 4 4 4 1 4 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 4 4 4 4 4 4 4 4 1 1 3 1 1 1 4 4 1 2 3 4 2 4 4 4 4 2 1 1 3 4 3 3 1 4 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 4 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 1 4 4 3 4 4 4 3 3 4 4 3 4 4 4 4 3 3 1 3 4 3 3 4 3 3 4 4 3 4 3 4 3 4 3 3 4 4 4 4 4 3 1 4 4 4 3 4 3 3 4 3 1 1 4 3 3 3 3 4 4 3 4 4 4 3 3 1 1 3 3 3 3 3 3 3 3 3 3 3 3 1 4 2 2 3 1 2 3 2 3 1 4 3 1 4 4 3 2 2 3 2 4 4 2 4 4 4 4 4 4 4 3 4 4 3 4 3 3 3 3 4 4 3 3 3 3 3 3 3 3 3 3 4 4 3 3 4 1 1 3 4 4 2 2 1 4 3 4 3 2 1 4 4 3 4 4 4 3 3 3 4 3 4 4 4 4 1 2 3 3 2 4 4 4 4 4 4 4 4 4 4 3 4 3 2 4 4 4 4 4 3 4 4 4 4 4 4 3 3 1 3 4 2 4 4 4 3 4 4 4 4 4 4 4 4 4 4 1 1 1 4 4 3 3 4 1 3 3 3 3 4 4 2 4 2 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 3 4 3 4 3 4 4 4 4 4 3 4 4 1 1 4 4 4 4 4 4 4 4 3 4 3 4 4 3 4 3 3 3 4 4 1 1 4 4 4 4 4 4 4 3 4 4 3 4 1 2 1 1 4 4 4 4 4 3 4 3 3 4 2 2 2 2 4 4 4 2 2 1 1 4 4 4 2 4 1 3 1 1 3 4 1 3 3 3 4 2 4 4 4 4 2 4 4 4 1 3 4 3 4 4 4 1 4 4 4 4 2 2 4 4 3 2 3 4 4 4 4 4 4 4 4 3 3 3 3 3 4 4 4 2 3 3 3 4 3 3 2 2 4 4 2 4 2 2 3 4 3 3 4 2 4 4 4 2 3 2 2 2 3 4 3 4 4 2 1 4 4 3 4 4 4 1 1 2 2 2 3 3 1 1 4 1 3 4 4 2 2 2 3 2 2 4 4 4 4 4 4 4 4 4 4 4 2 4 2 4 2 3 3 3 4 3 4 4 4 4 4 4 4 4 4 1 4 4 3 3 4 3 3 4 4 3 3 3 3 3 3 4 3 3 1 1 4 2 4 3 4 2 2 3 4 3 3 3 3 3 3 3 1 4 4 2 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 4 2 4 4 4 4 4 4 2 2 2 2 1 1 4 4 1 1 4 1 4 4 4 4 2 1 2 2 4 4 4 4 4 4 4 1 4 2 4 4 1 1 4 2 4 4 2 2 4 4 4 4 3 1 4 1 1 1 1 1 1 4 2 1 2 4 1 1 1 1 4 1 1 4 4 1 4 4 4 4 4 2 4 1 4 4 4 3 4 4 4 4 4 4 4 4 4 4 3 3 2 1 4 4 1 4 4 4 4 2 2 4 4 4 1 1 1 1 1 4 4 1 4 4 1 1 4 1 1 4 1 1 4 4 4 4 4 1 1 1 4 1 1 1 3 4 1 4 4 4 1 4 1 4 4 4 1 4 4 4 1 1 4 1 1 4 1 1 1 1 4 1 4 4 4 4 2 4 4 1 1 4 1 1 1 1 4 4 4 1 1 1 4 4 2 1 1 4 4 1 4 4 4 1 1 1 1 4 1 2 2 4 4 4 4 4 4 4 4 4 2 4 1 1 4 1 1 1 1 1 3 3 4 3 4 4 4 4 1 1 1 1 1 1 1 4 4 4 1 4 1 4 3 1 1 4 4 4 1 1 1 4 2 2 4 1 2 2 4 4 4 4 4 4 4 2 4 4 4 4 4 4 1 4 1 4 4 1 4 4 4 4 4 4 1 4 4 4 4 4 4 4 4 4 3 1 4 1 2 1 2 1 2 3 4 2 4 4 3 1 1 1 2 2 4 2 4 4 4 4 3 4 4 4 3 2 2 4 1 1 4 1 1 1 1 1 4 4 4 4 4 2 4 2 1 1 4 4 4 4 1 1 4 4 4 4 4 4 4 4 game_score : 650\n"
     ]
    }
   ],
   "source": [
    "# playing using the online q-net\n",
    "\n",
    "import time\n",
    "\n",
    "#sess = tf.InteractiveSession()\n",
    "#saver.restore(sess, './datasets/Pacman/pacman.ckpt')\n",
    "\n",
    "n_max_steps = 5000\n",
    "frames = [] # will store the game frames, can be converted to video\n",
    "game_reward = 0\n",
    "\n",
    "# start a  new game\n",
    "obs = env.reset()\n",
    "\n",
    "# skip the start of each life as in pacman its just waiting\n",
    "for skip in range(skip_start): \n",
    "    obs, reward, done, info = env.step(0)\n",
    "\n",
    "lives_left = 3\n",
    "\n",
    "for stp in range(n_max_steps):\n",
    "    state = preprocess(obs)    \n",
    "    # determine the action using online q_net\n",
    "    q_vals_eval = q_vals_online.eval(feed_dict={X_state: state.reshape(1, input_height, input_width, n_channels)})[0]\n",
    "    action = np.argmax(q_vals_eval)\n",
    "    # execute the action\n",
    "    obs, reward, done, info = env.step(1 + action)\n",
    "    \n",
    "    # skip the start of each life as in pacman its just waiting\n",
    "    if lives_left != info['ale.lives']:\n",
    "        lives_left = info['ale.lives']\n",
    "        for skip in range(35): \n",
    "            obs, reward, done, info = env.step(0)\n",
    "    \n",
    "    print(1 + action, end=' ')\n",
    "    frames.append(obs) # if you want to store the game as a vid\n",
    "    game_reward += int(reward)\n",
    "    env.render()\n",
    "    time.sleep(0.05) # to slowdown the game\n",
    "    # check if game ended\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print('game_score :', game_reward)\n",
    "# 0:no change, 1:t, 2:r, 3:l, 4:d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close the rendering window\n",
    "\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./datasets/Pacman/dec5/pacman.ckpt\n"
     ]
    }
   ],
   "source": [
    "# restore a saved model\n",
    "\n",
    "saver.restore(sess, './datasets/Pacman/dec5/pacman.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCAkRBASIggJEYSECEJCBKH/AK1gY4otiF0zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9881d8c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# saving the previous game as a video\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return anime.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)\n",
    "\n",
    "plot_animation(frames).save('PacMan.mp4', fps=30, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "\n",
    "# keep only 4 actions: l, r, u, d\n",
    "# if timer=8 is not used it'll never see states that are far spread out\n",
    "\n",
    "# chosing a random action after every 8 steps (and using it for 8 steps) gives a mean reward of 470.5\n",
    "# mean rewards of various timer vals\n",
    "# {timer: mean rwd} {3: 339.0833333333333, 4: 385.25, 5: 418.8333333333333, 6: 425.8333333333333, 7: 456.6666666666667, 8: 470.5, 9: 446.6666666666667}\n",
    "\n",
    "#cnn, channels = 3, L1 LOSS, learning_rate = 0.001 n_steps_train = 15 n_steps_copy = 3000 discount_rate = 0.99999 batch_size = 5 \n",
    "#skip_start = 85 training_start = 60  timer = 8, GradDesc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
